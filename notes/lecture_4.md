# Learning

## Why Learning

1. Some model might not be available. In this case, it can be built up over time
   by some other algorithm. An example would be to train a deep learning model;
   there is no _specific_ version of the model available to the programmers, but
   we can adapt a general model to our specific purposes.

2. ...

3. Not understanding the problem.

## What is learning

While there are many approaches to learning, some of which might be more
useful/correct in certain contexts than others. Here, we focus on **learning as
induction**. How to use induction to learn is then dependent on the technique
that is used to implement some agent.

Thus, while were are not explicitly building a model, we provide information to
some statistical model to then make decisions. It is therefore important to
decided what kind of information we should provide the model for it to make the
right decision. Not only that, we also have to quantify what is the right
decision.

We can split the space of what is the 'right' decision into two philosophies,
the **Idealistic** and **Pragmatic** approach. The idealistic attempts to stay
true to the way the world works, making observations and then using statistical
models to learn. The pragmatic approach, however, simply considers "whatever
improves the performance".

While the idealistic perspective would have great accuracy, the tractability,
however, is at odds. As the world is incredibly complex, the search space for
hypothesis (often denoted as $H$) is also _huge_. Therefore, assumptions often
need to be made, which brings most, if not any, model somewhere in between the
two extremes.

## Formalizing Learning

- Learning in Bayesian Networks can take a couple of forms:

  1. **Bayesian Learning**: Is done by computing the posterior $P(H|o)$ which
     uses the prior $P(h)$ and can be used in a weighted variant (utility) to
     determine the best 'action', $V(a) = \sum_h P(H|d)u(a, h)$

     - Bayesian learning is optimal, but untractable with realistic scenarios
       as the hypothesis space is often incredibly large.

  2. **Maximum Likelihood**: The most likely hypothesis, $h_{ML}$ will be
     $h_{ML} = \max_h P(d|h)$. Then we can select and action according to
     $V(a) = u(a, h_{ML})$. It is, however, prone to overfitting.

     - Instead of computing the default maximum likelihood, we often compute the
       maximum **log likelihood**. As the logarithm is a monotonically
       increasing function, it should preserve the maxima we are looking for.

     - For example, the equation $P(d|h_\theta) = \theta^c \cdot (1 -
       \theta)^l$, where $c$ is the number of cherry candies, $l$ the number of
       lime candies and $\theta$ the probability of encountering the former, can
       be solved in a more 'computationally friendly' manner by taking the log,
       $L(d|h_\theta) = c \log \theta + l \log (1-\theta)$. We can then find the
       maximum by taking the derivative and finding $0$, $\frac{d
       L(d|h_\theta)}{d\theta}=0$. This will result in $\theta = \frac{c}{c +
       l}$, not very surprising.

  3. **Maximum A Posteriori (MAP)**: Which is just the **ML**, but also taking
     into consideration the prior of the hypothesis: $h_{MAP} = \max_h
     P(d|h)P(h)$.

     - Here, the prior (just like in Bayesian Learning) penalizes the hypothesis
       being too complex. We regard more complex hypotheses being less likely.

- To be clear, this is distinct from **Most Likely Sequence Estimation**
  because there, we attempt to determine what happened, but the model itself
  doesn't necessarily improve. We learn about hidden states, but nothing about
  the model. ==This is my current hypothesis as of lecture 5==

### The EM Algorithm

- Before, we only considered learning when we had access to data about _all_ the
  relevant variables. Now, we consider learning not only the observable data,
  but also hidden or **latent variables**.

- To solve such learning problems, one can use the **Expectation Maximization**
  algorithm, or, **EM Algorithm**.

- Take the problem of learning **mixture of Gaussians** (we will later see how
  this is relevant to latent variables). That is, we have a **mixture
  distrbution**, $P$, with $k$ **components**:

$$
P(x) = \sum_k P(C=i)P(x|C=i)
$$

- The problem then is, _determine the center, $\mu_i$, of each Gaussian and the
  covariance, $\Sigma_i$, of each component, $i$_.

- The EM algorithm will provide exactly this and works according to a two-step
  process.

  1. **E-step**: Compute probabilities $p_{ij} = P(C=i|x_j)$, the probability
     that datum $x_j$ was generated by component $i$. By Bayes' rule, we have
     $p_{ij}=\alpha P(x_j|C=i)P(C=i)$. The term $P(x_j|C=i)$ is just the
     probabiltiy at $x_j$ for the $i$th Gaussian. Define $n_i=\sum_j p_{ij}$,
     the effective number of data points currently assigned to component $i$.
  2. **M-step**: Compute the new mean, covariance, and component weights using
     the following steps in sequence:

$$
\begin{split}
   \mu_i &\leftarrow \sum_j p_ij x_j / n_i \\
   \Sigma_i &\leftarrow \sum_j p_ij (x_j - \mu_i)(x_j - \mu_i)^T / n_i \\
   w_i &\leftarrow n_i / N
\end{split}
$$
